# -*- coding: utf-8 -*-
"""NER-simple_transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/177vR9G1IAin8OTK5NLwtcEOknSRzJXYu
"""

#! rm -r sample_data

#!pip install simpletransformers

from simpletransformers.ner import NERModel, NERArgs
import pandas as pd
import torch
from torch import multiprocessing
from torch.utils.data import Dataset, DataLoader
import nltk
nltk.download('punkt')

from nltk.tokenize import sent_tokenize

############  Define Dataset ###################################################
"""
train_data = [
    [0, "Harry", "B-PER"],
    [0, "Potter", "I-PER"],
    [0, "was", "O"],
    [0, "a", "O"],
    [0, "student", "B-MISC"],
    [0, "at", "O"],
    [0, "Hogwarts", "B-LOC"],
    [1, "Albus", "B-PER"],
    [1, "Dumbledore", "I-PER"],
    [1, "founded", "O"],
    [1, "the", "O"],
    [1, "Order", "B-ORG"],
    [1, "of", "I-ORG"],
    [1, "the", "I-ORG"],
    [1, "Phoenix", "I-ORG"],
]
train_data = pd.DataFrame(
    train_data, columns=["sentence_id", "words", "labels"]
)
"""

custom_labels = ["O", "ORG", "B-ORG", "I-ORG", "NORM", "B-NORM","I-NORM", "PROC", "B-PROC", "I-PROC", "MOD", "B-MOD", "I-MOD", "SPEC", "B-SPEC", "I-SPEC", "MED", "B-MED", "I-MED", "MAT", "SEC"]


train_data = '/content/train.conll'

flag_classes = 0

###############################################################################

#################################### TRAINING the MODEL ################################################################
Train = False

def run():
    torch.multiprocessing.freeze_support()
  
    # defining model arguments
    model_args = NERArgs()
    model_args.overwrite_output_dir = True
    model_args.no_save = False
    model_args.num_train_epochs = 15
    model_args.max_seq_length = 128

    # model defined here
    #model = NERModel("bert", "bert-base-german-dbmdz-uncased", args=model_args, use_cuda=True, labels=custom_labels)
    model = NERModel("bert", "bert-base-german-cased", args=model_args, use_cuda=True, labels=custom_labels)

    # training starts here
    model.train_model(train_data, show_running_loss=True, verbose=1, overwrite_output_dir=True)

if Train:
  if __name__ == '__main__':
      run()

#######################################################################################################################

######################## LOADING THE MODEL #########################

model_args = NERArgs()
model_args.max_seq_length = 128

model = NERModel("bert", "/content/drive/MyDrive/ner/outputs", use_cuda=True, args=model_args, labels=custom_labels)
print("model is loaded from disk")

########### Evaluating the model ##############################################

def run():
    torch.multiprocessing.freeze_support()
    print(" Evaluating the model....")

    eval_data = '/content/final_eval_file.conll'
    result, model_outputs, preds = model.eval_model(eval_data)

    print(result)
    print(model_outputs)
    print(preds)

if __name__ == '__main__':
    run()

################################################################################

############### Making Predictions with the Model ##############################


print("making predictions...")

pred_array = []
sequence = ''

# Read from a text file / pdf file line by line, give it to a string called "sequence", feed sequence to the model
# OR
# Read the whole text/pdf document into a string (append or concatenate) and then pass sequence to dataloader for 128 bs.

file1 = open("/content/drive/MyDrive/ner/test_set_lower_cased.txt","r")
sequence=file1.read()
#print(sequence)



"""
###split words
sequence = sequence.split()
### Batching the data as BERT only takes max 512 tokens at a time

data = DataLoader(sequence, batch_size = 10)
# Imp Note : the batch_size must be smaller than the number of words in the sequence !
"""


def run():
    torch.multiprocessing.freeze_support()
    # we use NLTK sentence tokenizer to split sentences using full stops and then pass each sentence to BERT
    for i in sent_tokenize(sequence):
      #print('§§',i)
      predictions, raw_outputs = model.predict([i])
      #print(predictions)
      #print(raw_outputs)
      pred_array.append(predictions)
    
    # the dataloader approach has been avoided as it trunactes the sentence at arbitrary positions
    """
    for i in data:
        #print(i)
        sent = ' '.join(i) # join to make a string from the list
        #print(sent, "§§§")
        # making predictions
        predictions, raw_outputs = model.predict([sent])
        #print(predictions)
        #print(raw_outputs)
        pred_array.append(predictions)
    """
    
    print('Final Predictions are:')
    print(pred_array)
    #print('flag still 0')



if flag_classes == 0:
  if __name__ == '__main__':
      run()

################################################################################
###############   EXTRACTING CERTAIN CLASSES FROM OUR OUTPUT ###################

flag_classes = 1
#print('now flag set to 1')

if flag_classes:
  classes = ["ORG", "B-ORG", "I-ORG", "NORM", "B-NORM","I-NORM", "PROC", "B-PROC", "I-PROC", "MOD", "B-MOD", "I-MOD", "SPEC", "B-SPEC", "I-SPEC", "MED", "B-MED", "I-MED", "MAT", "SEC"]

  org_ls =  []
  norm_ls = []
  proc_ls = []
  mod_ls =  []
  spec_ls = []
  med_ls =  []
  mat_ls =  []
  sec_ls =  []

  flag_org = 0
  flag_norm = 0
  flag_proc = 0
  flag_mod = 0
  flag_spec = 0
  flag_med = 0
  flag_mat = 0
  flag_sec = 0



  print("Final Predictions")
  print(pred_array,'\n') # pred_array is a list of lists

  for i in pred_array:
    #print(i[0]) #i[0] is a list of dictionaries 
    #print(type(i[0]))
    # j gives us all dictionaries in i[0]
    
    for j in i[0]:
      
      for key, value in j.items():  # we unpack keys and values from a DICT
        #if value in classes:
        
        if value == 'B-ORG' or value == 'I-ORG' or value == 'ORG':
          #print('found organization')
          flag_org = 1
          org_ls.append(key)
          org_x = set(org_ls)
        
        elif value == 'B-NORM' or value == 'I-NORM' or value == 'NORM':
          #print('found NORMS')
          flag_norm = 1
          norm_ls.append(key)
          norm_x = set(norm_ls)

        elif value == 'B-PROC' or value == 'I-PROC' or value == 'PROC':
          #print('found processes')
          flag_proc = 1
          proc_ls.append(key)
          proc_x = set(proc_ls)

        elif value == 'B-MOD' or value == 'I-MOD' or value == 'MOD':
          #print('found modules')
          flag_mod = 1
          mod_ls.append(key)
          mod_x = set(mod_ls)

        elif value == 'B-SPEC' or value == 'I-SPEC' or value == 'SPEC':
          #print('found specifications')
          flag_spec = 1
          spec_ls.append(key)
          spec_x = set(spec_ls)

        elif value == 'B-MED' or value == 'I-MED' or value == 'MED':
          #print('found median')
          flag_med = 1
          med_ls.append(key)
          med_x = set(med_ls)

        elif value == 'MAT':
          #print('found materials')
          flag_mat = 1
          mat_ls.append(key)
          mat_x = set(mat_ls)
    
        elif value == 'SEC':
          #print('found security')
          flag_sec = 1
          sec_ls.append(key)
          sec_x = set(sec_ls)

  ## only print those sets which are non-empty!
  if flag_org == 1:
    print("****************  Organizations ********************")
    print(org_x, '\n')
  if flag_norm == 1:
    print("****************  Standards ********************")
    print(norm_x, '\n')
  if flag_proc == 1:
    print("****************  Processes ********************")
    print(proc_x, '\n')
  if flag_mod == 1:
    print("****************  Modules/Assembly Parts ********************")
    print(mod_x, '\n')
  if flag_spec == 1:
    print("****************  Specifications ********************")
    print(spec_x, '\n')
  if flag_med == 1:
    print("****************  Medien ********************")
    print(med_x, '\n')
  if flag_mat == 1:
    print("****************  Materials ********************")
    print(mat_x, '\n')
  if flag_sec == 1:
    print("****************  Security ********************")
    print(sec_x, '\n')

#mv /content/outputs /content/drive/MyDrive/ner

#mv /content/test_set_lower_cased.txt /content/drive/MyDrive/ner